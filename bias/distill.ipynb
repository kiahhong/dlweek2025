{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub\n",
    "%pip install transformers\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers datasets torch\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "download_folder = '.'\n",
    "path = kagglehub.dataset_download(\"surajkarakulath/labelled-corpus-political-bias-hugging-face\")\n",
    "\n",
    "data_dir = \"/root/.cache/kagglehub/datasets/surajkarakulath/labelled-corpus-political-bias-hugging-face/versions/1\"\n",
    "label_mapping = {\"Left Data\": 0, \"Center Data\": 1, \"Right Data\": 2}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for bias_label in [\"Left Data\", \"Center Data\", \"Right Data\"]:\n",
    "            bias_path = os.path.join(data_dir, bias_label, bias_label)\n",
    "            label = label_mapping[bias_label]\n",
    "            \n",
    "            for txt_file in os.listdir(bias_path):\n",
    "                if txt_file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(bias_path, txt_file)\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read()\n",
    "                    self.data.append((text, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512\n",
    "        )\n",
    "        encoding.pop(\"token_type_ids\", None)  \n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}, torch.tensor(label)\n",
    "\n",
    "dataset = BiasDataset(data_dir, tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\"bucketresearch/politicalBiasBERT\")\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=3)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=4.0):\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    loss = F.kl_div(student_probs, teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "    return loss\n",
    "\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    student_model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        inputs, labels = batch\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        student_outputs = student_model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        loss_ce = F.cross_entropy(student_logits, labels)\n",
    "        loss_kd = distillation_loss(student_logits, teacher_logits)\n",
    "        loss = 0.5 * loss_ce + 0.5 * loss_kd\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "student_accuracy = evaluate_model(student_model, test_dataloader)\n",
    "\n",
    "student_model.save_pretrained(\"distilled_biasBERT\")\n",
    "tokenizer.save_pretrained(\"distilled_biasBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
